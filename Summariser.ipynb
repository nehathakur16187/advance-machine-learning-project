{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-04-27T14:27:24.935192Z","iopub.status.busy":"2024-04-27T14:27:24.934909Z","iopub.status.idle":"2024-04-27T14:27:38.597000Z","shell.execute_reply":"2024-04-27T14:27:38.595055Z","shell.execute_reply.started":"2024-04-27T14:27:24.935158Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: rouge_score in c:\\users\\stoke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.1.2)\n","Requirement already satisfied: absl-py in c:\\users\\stoke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rouge_score) (2.1.0)\n","Requirement already satisfied: nltk in c:\\users\\stoke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rouge_score) (3.8.1)\n","Requirement already satisfied: numpy in c:\\users\\stoke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rouge_score) (1.26.4)\n","Requirement already satisfied: six>=1.14.0 in c:\\users\\stoke\\appdata\\roaming\\python\\python312\\site-packages (from rouge_score) (1.16.0)\n","Requirement already satisfied: click in c:\\users\\stoke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk->rouge_score) (8.1.7)\n","Requirement already satisfied: joblib in c:\\users\\stoke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk->rouge_score) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in c:\\users\\stoke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk->rouge_score) (2024.5.15)\n","Requirement already satisfied: tqdm in c:\\users\\stoke\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk->rouge_score) (4.66.4)\n","Requirement already satisfied: colorama in c:\\users\\stoke\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk->rouge_score) (0.4.6)\n","Note: you may need to restart the kernel to use updated packages.\n"]},{"name":"stderr","output_type":"stream","text":["\n","[notice] A new release of pip is available: 24.1.2 -> 24.2\n","[notice] To update, run: python.exe -m pip install --upgrade pip\n"]}],"source":["pip install rouge_score"]},{"cell_type":"markdown","metadata":{},"source":["# Import Libraries"]},{"cell_type":"code","execution_count":56,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-04-27T14:59:40.267288Z","iopub.status.busy":"2024-04-27T14:59:40.266384Z","iopub.status.idle":"2024-04-27T14:59:40.488311Z","shell.execute_reply":"2024-04-27T14:59:40.487364Z","shell.execute_reply.started":"2024-04-27T14:59:40.267247Z"},"trusted":true},"outputs":[],"source":["from transformers import (AutoModelForSeq2SeqLM,\n","                          AutoTokenizer,\n","                          DataCollatorForSeq2Seq,\n","                          Seq2SeqTrainer,\n","                          Seq2SeqTrainingArguments,\n","                          EarlyStoppingCallback,\n","                          GenerationConfig)\n","from datasets import Dataset, DatasetDict, load_metric\n","import pandas as pd\n","import numpy as np\n","import torch\n","import nltk\n","\n","nltk.download(\"punkt\", quiet=True)\n","\n","# Load metric\n","metric = load_metric(\"rouge\", trust_remote_code=True)\n"]},{"cell_type":"code","execution_count":57,"metadata":{"execution":{"iopub.execute_input":"2024-04-27T14:34:15.867900Z","iopub.status.busy":"2024-04-27T14:34:15.867476Z","iopub.status.idle":"2024-04-27T14:34:15.872791Z","shell.execute_reply":"2024-04-27T14:34:15.871695Z","shell.execute_reply.started":"2024-04-27T14:34:15.867869Z"},"trusted":true},"outputs":[],"source":["# Device setup\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# Parameters\n","encoder_max_length = 512\n","decoder_max_length = 128\n","batch_size = 2"]},{"cell_type":"markdown","metadata":{},"source":["# Prepare Data, Model, and Tokenizer"]},{"cell_type":"code","execution_count":58,"metadata":{"execution":{"iopub.execute_input":"2024-04-27T14:27:47.768630Z","iopub.status.busy":"2024-04-27T14:27:47.768194Z","iopub.status.idle":"2024-04-27T14:27:48.889287Z","shell.execute_reply":"2024-04-27T14:27:48.888402Z","shell.execute_reply.started":"2024-04-27T14:27:47.768592Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\stoke\\AppData\\Local\\Temp\\ipykernel_19144\\79807151.py:1: DtypeWarning: Columns (0,1,2) have mixed types. Specify dtype option on import or set low_memory=False.\n","  train = pd.read_csv(\"./dataset/train.csv\")\n"]}],"source":["train = pd.read_csv(\"./dataset/train.csv\")\n","train_dataset = Dataset.from_pandas(train)"]},{"cell_type":"code","execution_count":59,"metadata":{"execution":{"iopub.execute_input":"2024-04-27T14:29:12.553577Z","iopub.status.busy":"2024-04-27T14:29:12.552642Z","iopub.status.idle":"2024-04-27T14:29:15.714432Z","shell.execute_reply":"2024-04-27T14:29:15.713549Z","shell.execute_reply.started":"2024-04-27T14:29:12.553533Z"},"trusted":true},"outputs":[],"source":["cheakPoint = \"facebook/bart-large-cnn\"\n","model = AutoModelForSeq2SeqLM.from_pretrained(cheakPoint).to(device)\n","tokenizer = AutoTokenizer.from_pretrained(cheakPoint)"]},{"cell_type":"code","execution_count":60,"metadata":{"execution":{"iopub.execute_input":"2024-04-27T14:29:37.190134Z","iopub.status.busy":"2024-04-27T14:29:37.189708Z","iopub.status.idle":"2024-04-27T14:29:37.221977Z","shell.execute_reply":"2024-04-27T14:29:37.219569Z","shell.execute_reply.started":"2024-04-27T14:29:37.190100Z"},"trusted":true},"outputs":[],"source":["# Split dataset\n","train_dataset = train_dataset.shuffle(seed=42)\n","train, val = train_dataset.select(range(400)), train_dataset.select(range(400, 490))\n","dataset_dict = DatasetDict({\"train\": train, \"validation\": val})\n","dataset_dict = dataset_dict.remove_columns(\"id\")"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Filter:   0%|          | 0/400 [00:00<?, ? examples/s]"]},{"name":"stderr","output_type":"stream","text":["Filter: 100%|██████████| 400/400 [00:00<00:00, 8305.31 examples/s]\n","Filter: 100%|██████████| 90/90 [00:00<00:00, 21304.10 examples/s]\n"]}],"source":["# Filter out rows with None values\n","def filter_none_rows(dataset):\n","    def is_not_none(example):\n","        return example[\"article\"] is not None and example[\"highlights\"] is not None\n","    return dataset.filter(is_not_none)\n","\n","train_filtered = filter_none_rows(train)\n","val_filtered = filter_none_rows(val)"]},{"cell_type":"markdown","metadata":{},"source":["# Tokenization Step"]},{"cell_type":"code","execution_count":62,"metadata":{"execution":{"iopub.execute_input":"2024-04-27T14:32:33.757734Z","iopub.status.busy":"2024-04-27T14:32:33.757049Z","iopub.status.idle":"2024-04-27T14:32:33.765695Z","shell.execute_reply":"2024-04-27T14:32:33.764486Z","shell.execute_reply.started":"2024-04-27T14:32:33.757698Z"},"trusted":true},"outputs":[],"source":["# Tokenization Step\n","def batch_tokenize_preprocess(batch, tokenizer, encoder_max_length, decoder_max_length):\n","    source = batch[\"article\"]\n","    target = batch[\"highlights\"]\n","\n","    # Ensure that source and target are lists of strings\n","    if isinstance(source, str):\n","        source = [source]\n","    if isinstance(target, str):\n","        target = [target]\n","\n","    # Tokenize the source and target\n","    source_tokenized = tokenizer(source, padding=\"max_length\", truncation=True, max_length=encoder_max_length)\n","    target_tokenized = tokenizer(target, padding=\"max_length\", truncation=True, max_length=decoder_max_length)\n","\n","    # Ignore padding in the loss\n","    target_labels = [\n","        [-100 if token == tokenizer.pad_token_id else token for token in l]\n","        for l in target_tokenized[\"input_ids\"]\n","    ]\n","\n","    # Create a dictionary for the batch\n","    batch_dict = {\n","        \"input_ids\": source_tokenized[\"input_ids\"],\n","        \"attention_mask\": source_tokenized[\"attention_mask\"],\n","        \"labels\": target_labels,\n","    }\n","\n","    return batch_dict"]},{"cell_type":"code","execution_count":89,"metadata":{"execution":{"iopub.execute_input":"2024-04-27T14:34:21.455215Z","iopub.status.busy":"2024-04-27T14:34:21.454784Z","iopub.status.idle":"2024-04-27T14:34:22.528599Z","shell.execute_reply":"2024-04-27T14:34:22.527628Z","shell.execute_reply.started":"2024-04-27T14:34:21.455177Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\n","Map: 100%|██████████| 15/15 [00:00<00:00, 176.36 examples/s]\n","\n","Map: 100%|██████████| 4/4 [00:00<00:00, 258.13 examples/s]"]},{"name":"stdout","output_type":"stream","text":["Dataset({\n","    features: ['article', 'highlights', 'input_ids', 'attention_mask', 'labels'],\n","    num_rows: 5\n","})\n","Dataset({\n","    features: ['article', 'highlights', 'input_ids', 'attention_mask', 'labels'],\n","    num_rows: 4\n","})\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Apply tokenization to the data\n","train_data = train_filtered.map(\n","    lambda batch: batch_tokenize_preprocess(batch, tokenizer, encoder_max_length, decoder_max_length),\n","    batched=True,\n","    remove_columns=[\"id\"]\n",")\n","\n","validation_data = val_filtered.map(\n","    lambda batch: batch_tokenize_preprocess(batch, tokenizer, encoder_max_length, decoder_max_length),\n","    batched=True,\n","    remove_columns=[\"id\"]\n",")\n","\n","# Verify tokenized data\n","print(train_data.select(range(min(5, train_data.num_rows))))\n","print(validation_data.select(range(min(5, validation_data.num_rows))))"]},{"cell_type":"markdown","metadata":{},"source":["# *Metric func* for compute metrics at evaluation. "]},{"cell_type":"code","execution_count":90,"metadata":{"execution":{"iopub.execute_input":"2024-04-27T14:34:59.480077Z","iopub.status.busy":"2024-04-27T14:34:59.479632Z","iopub.status.idle":"2024-04-27T14:34:59.490369Z","shell.execute_reply":"2024-04-27T14:34:59.489249Z","shell.execute_reply.started":"2024-04-27T14:34:59.480046Z"},"trusted":true},"outputs":[],"source":["def compute_metrics(eval_preds):\n","    preds, labels = eval_preds\n","\n","    # Print shapes for debugging\n","    print(f\"Preds shape: {preds.shape}\")\n","    print(f\"Labels shape: {labels.shape}\")\n","\n","    if isinstance(preds, tuple):\n","        preds = preds[0]\n","\n","    # Ensure preds is 2D or 3D\n","    if len(preds.shape) == 2:\n","        preds = np.argmax(preds, axis=-1)\n","    elif len(preds.shape) == 3:\n","        preds = np.argmax(preds, axis=-1)  # Logits might be 3D\n","    else:\n","        raise ValueError(\"Predictions have an unexpected shape.\")\n","\n","    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n","\n","    # Handle -100 tokens in labels for padding\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n","    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n","\n","    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n","    result[\"gen_len\"] = np.mean(prediction_lens)\n","    result = {k: round(v, 4) for k, v in result.items()}\n","    return result\n"]},{"cell_type":"code","execution_count":93,"metadata":{"execution":{"iopub.execute_input":"2024-04-27T14:34:59.973614Z","iopub.status.busy":"2024-04-27T14:34:59.972604Z","iopub.status.idle":"2024-04-27T14:34:59.978490Z","shell.execute_reply":"2024-04-27T14:34:59.977347Z","shell.execute_reply.started":"2024-04-27T14:34:59.973576Z"},"trusted":true},"outputs":[],"source":["data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"]},{"cell_type":"markdown","metadata":{},"source":["# Prepare the module"]},{"cell_type":"code","execution_count":94,"metadata":{"execution":{"iopub.execute_input":"2024-04-27T14:38:41.689978Z","iopub.status.busy":"2024-04-27T14:38:41.689546Z","iopub.status.idle":"2024-04-27T14:38:41.730297Z","shell.execute_reply":"2024-04-27T14:38:41.729074Z","shell.execute_reply.started":"2024-04-27T14:38:41.689947Z"},"trusted":true},"outputs":[],"source":["# Training arguments\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=\"./results\",\n","    evaluation_strategy=\"steps\",\n","    eval_steps=40,\n","    save_steps=1e6,\n","    num_train_epochs=5,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    warmup_steps=500,\n","    weight_decay=0.01,\n","    logging_dir='./logs',\n","    logging_steps=10,\n","    save_total_limit=3,\n","    predict_with_generate=True,\n","    learning_rate=5e-5,\n","    load_best_model_at_end=True,\n",")"]},{"cell_type":"code","execution_count":95,"metadata":{"execution":{"iopub.execute_input":"2024-04-27T14:38:43.936544Z","iopub.status.busy":"2024-04-27T14:38:43.935748Z","iopub.status.idle":"2024-04-27T14:38:43.964671Z","shell.execute_reply":"2024-04-27T14:38:43.963520Z","shell.execute_reply.started":"2024-04-27T14:38:43.936511Z"},"trusted":true},"outputs":[],"source":["# Initialize the Trainer\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_data,\n","    eval_dataset=validation_data,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n","    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",")\n"]},{"cell_type":"markdown","metadata":{},"source":["# Fine-Tuning step"]},{"cell_type":"code","execution_count":96,"metadata":{"execution":{"iopub.execute_input":"2024-04-27T14:38:45.886183Z","iopub.status.busy":"2024-04-27T14:38:45.885773Z","iopub.status.idle":"2024-04-27T14:47:44.285612Z","shell.execute_reply":"2024-04-27T14:47:44.284516Z","shell.execute_reply.started":"2024-04-27T14:38:45.886146Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 40/40 [34:01<00:00, 51.05s/it]\n"," 25%|██▌       | 10/40 [03:04<09:01, 18.06s/it]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 1.0445, 'grad_norm': 5.121329307556152, 'learning_rate': 1.0000000000000002e-06, 'epoch': 1.25}\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 20/40 [06:08<06:12, 18.64s/it]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 1.0257, 'grad_norm': 10.177864074707031, 'learning_rate': 2.0000000000000003e-06, 'epoch': 2.5}\n"]},{"name":"stderr","output_type":"stream","text":[" 75%|███████▌  | 30/40 [09:11<03:08, 18.81s/it]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.8349, 'grad_norm': 6.7616286277771, 'learning_rate': 3e-06, 'epoch': 3.75}\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 40/40 [12:06<00:00, 16.65s/it]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.7034, 'grad_norm': 12.537304878234863, 'learning_rate': 4.000000000000001e-06, 'epoch': 5.0}\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Preds shape: (4, 142)\n","Labels shape: (4, 142)\n"]},{"name":"stderr","output_type":"stream","text":["\n","100%|██████████| 40/40 [13:00<00:00, 16.65s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 1.688814640045166, 'eval_rouge1': 0.0, 'eval_rouge2': 0.0, 'eval_rougeL': 0.0, 'eval_rougeLsum': 0.0, 'eval_gen_len': 1.0, 'eval_runtime': 53.1565, 'eval_samples_per_second': 0.075, 'eval_steps_per_second': 0.038, 'epoch': 5.0}\n"]},{"name":"stderr","output_type":"stream","text":["There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n","100%|██████████| 40/40 [13:06<00:00, 19.66s/it]"]},{"name":"stdout","output_type":"stream","text":["{'train_runtime': 786.5427, 'train_samples_per_second': 0.095, 'train_steps_per_second': 0.051, 'train_loss': 0.902126145362854, 'epoch': 5.0}\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"data":{"text/plain":["TrainOutput(global_step=40, training_loss=0.902126145362854, metrics={'train_runtime': 786.5427, 'train_samples_per_second': 0.095, 'train_steps_per_second': 0.051, 'total_flos': 81266422579200.0, 'train_loss': 0.902126145362854, 'epoch': 5.0})"]},"execution_count":96,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"code","execution_count":97,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"]},{"data":{"text/plain":["('./saved_model\\\\tokenizer_config.json',\n"," './saved_model\\\\special_tokens_map.json',\n"," './saved_model\\\\vocab.json',\n"," './saved_model\\\\merges.txt',\n"," './saved_model\\\\added_tokens.json',\n"," './saved_model\\\\tokenizer.json')"]},"execution_count":97,"metadata":{},"output_type":"execute_result"}],"source":["# Save the model\n","save_directory = './saved_model'\n","model.save_pretrained(save_directory)\n","tokenizer.save_pretrained(save_directory)"]},{"cell_type":"markdown","metadata":{},"source":["# Generate sumary"]},{"cell_type":"code","execution_count":98,"metadata":{"execution":{"iopub.execute_input":"2024-04-27T15:02:30.141658Z","iopub.status.busy":"2024-04-27T15:02:30.140794Z","iopub.status.idle":"2024-04-27T15:02:30.149508Z","shell.execute_reply":"2024-04-27T15:02:30.148498Z","shell.execute_reply.started":"2024-04-27T15:02:30.141611Z"},"trusted":true},"outputs":[],"source":["def generate_summary(test_samples, model, tokenizer, max_length=1024):\n","    inputs = tokenizer(test_samples, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n","    input_ids = inputs.input_ids.to(model.device)\n","    attention_mask = inputs.attention_mask.to(model.device)\n","\n","    # Set the decoder_start_token_id if not already set\n","    if model.config.decoder_start_token_id is None:\n","        model.config.decoder_start_token_id = tokenizer.pad_token_id or tokenizer.bos_token_id\n","\n","    # Generate summary\n","    outputs = model.generate(\n","        input_ids,\n","        attention_mask=attention_mask,\n","        num_beams=4,\n","        early_stopping=True\n","    )\n","\n","    # Decode the generated summaries\n","    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n","    return output_str"]},{"cell_type":"code","execution_count":101,"metadata":{"execution":{"iopub.execute_input":"2024-04-27T15:11:24.798535Z","iopub.status.busy":"2024-04-27T15:11:24.798055Z","iopub.status.idle":"2024-04-27T15:11:24.805831Z","shell.execute_reply":"2024-04-27T15:11:24.804754Z","shell.execute_reply.started":"2024-04-27T15:11:24.798501Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[\"Shrinking space on planes is putting our health and safety in danger, say experts. Consumer advisory group set up by Department of Transportation say it is putting passengers at risk. They say shrinking space on aeroplanes is not only uncomfortable - it's putting passengers' health in danger.\"]\n"]}],"source":["# Example usage\n","sample = \"Ever noticed how plane seats appear to be getting smaller and smaller? With increasing numbers of people taking to the skies, some experts are questioning if having such packed out planes is putting passengers at risk. They say that the shrinking space on aeroplanes is not only uncomfortable - it's putting our health and safety in danger. More than squabbling over the arm rest, shrinking space on planes putting our health and safety in danger? This week, a U.S consumer advisory group set up by the Department of Transportation said at a public hearing that while the government is happy to set standards for animals flying on planes, it doesn't stipulate a minimum amount of space for humans. 'In a world where animals have more rights to space and food than humans,' said Charlie Leocha, consumer representative on the committee. 'It is time that the DOT and FAA take a stand for humane treatment of passengers.' But could crowding on planes lead to more serious issues than fighting for space in the overhead lockers, crashing elbows and seat back kicking? Tests conducted by the FAA use planes with a 31 inch pitch, a standard which on some airlines has decreased . Many economy seats on United Airlines have 30 inches of room, while some airlines offer as little as 28 inches . Cynthia Corbertt, a human factors researcher with the Federal Aviation Administration, that it conducts tests on how quickly passengers can leave a plane. But these tests are conducted using planes with 31 inches between each row of seats, a standard which on some airlines has decreased, reported the Detroit News. The distance between two seats from one point on a seat to the same point on the seat behind it is known as the pitch. While most airlines stick to a pitch of 31 inches or above, some fall below this. While United Airlines has 30 inches of space, Gulf Air economy seats have between 29 and 32 inches, Air Asia offers 29 inches and Spirit Airlines offers just 28 inches. British Airways has a seat pitch of 31 inches, while easyJet has 29 inches, Thomson's short haul seat pitch is 28 inches, and Virgin Atlantic's is 30-31.\"\n","\n","res = generate_summary(sample, model, tokenizer, max_length=110)\n","print(res)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":1654566,"sourceId":2734496,"sourceType":"datasetVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":4}
